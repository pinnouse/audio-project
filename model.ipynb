{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def create_model(n_classes: int = 512) -> nn.Module:\n",
    "    \"\"\"Creates a new CNN.\n",
    "\n",
    "    Creates a new pytorch neural network that models the audio cortex\n",
    "    using the architecture presented by Josh McDermmott. One key difference\n",
    "    is that batch normalization in this model occurs over the whole minibatch\n",
    "    instead of the original 5 adjacent, zero-padded convolution window.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        n_classes: The number of classes the model should output to.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        A CNN model following Josh McDermmott's architecture.\n",
    "\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1, out_channels=96, kernel_size=9, stride=3, padding=3),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(96), # McDermmott does normalization differently*\n",
    "        nn.AvgPool2d(3, stride=2),\n",
    "        nn.Conv2d(96, out_channels=256, kernel_size=5, stride=2, padding=3),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.AvgPool2d(3, stride=2, padding=1),\n",
    "        nn.Conv2d(256, out_channels=512, kernel_size=3, stride=1, padding=2),\n",
    "        nn.Conv2d(512, out_channels=1024, kernel_size=3, stride=1, padding=2),\n",
    "        nn.Conv2d(1024, out_channels=512, kernel_size=3, stride=1, padding=2),\n",
    "        nn.AvgPool2d(3, stride=2),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(8*8*512, 4096),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(4096, n_classes),\n",
    "        nn.Softmax(dim=-1))\n",
    "\n",
    "model = create_model()\n",
    "X = torch.randn(1, 1, 256, 256)\n",
    "out = model(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optim = AdamW(model.parameters())\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def cv_sets(data: torch.Tensor, k: int = 10, test_size: float = 0.2,\n",
    "            random_seed: Optional[int] = None\n",
    "            ) -> List[Tuple[List[int], List[int]]]:\n",
    "    \"\"\"Generate cross-validation datasets.\n",
    "\n",
    "    Using scikit-learn's\n",
    "    [ShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html).\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        data: The data set to split upon.\n",
    "        k: Number of folds.\n",
    "        test_size: A float representing the portion of data to split into test.\n",
    "        random_seed: An integer for the random seed used to split the set.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        A list of `k` tuples, each tuple containing lists of train indices and\n",
    "        test indices.\n",
    "\n",
    "    \"\"\"\n",
    "    rs = ShuffleSplit(n_splits=k, random_state=random_seed, test_size=test_size)\n",
    "    return rs.split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def batcherize(X_set: torch.Tensor, y_set: torch.Tensor, indices: List[int],\n",
    "               batch_size: int) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    \"\"\"Split model into batches given a list of indices.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        X_set: A tensor for the input data into the model.\n",
    "        y_set: A tensor for the output data that the model learns to fit.\n",
    "        indices: A list of indices to create minibatches from.\n",
    "        batch_size: An integer batch size.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        A list of ceil(len(indices) / batch_size) to enumerate over, where each\n",
    "        element is a subset of the training input/target, split in the order\n",
    "        given by the indices argument.\n",
    "\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    n = math.ceil(len(indices) / batch_size)\n",
    "    for i in range(n):\n",
    "        start_ix = i * batch_size\n",
    "        idx = indices[start_ix:start_ix + batch_size]\n",
    "        batches.append(\n",
    "            (X_set[idx], y_set[idx])\n",
    "        )\n",
    "    return batches\n",
    "\n",
    "def train(model: nn.Module, epochs: int, X_train: torch.Tensor,\n",
    "          y_train: torch.Tensor, optim: torch.optim.Optimizer, loss_fn: any,\n",
    "          k: int = 10, bs: int = 16, test_size: float = 0.2\n",
    "          ) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"Train a model given parameters.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        model: A model instance to be trained.\n",
    "        epochs: The number of epochs to train for.\n",
    "        X_train: A pytorch tensor for the data to train on as input to the\n",
    "            model.\n",
    "        y_train: A pytorch tensor representing logits (indices) that the model\n",
    "            should output, corresponding to the input X_train.\n",
    "        optim: A pytorch optimizer for stepping the gradients.\n",
    "        loss_fn: The loss function to which the model is optimizing for,\n",
    "            typically CrossEntropyLoss for classification.\n",
    "        k: An integer number of folds for cross-validation training.\n",
    "        bs: The batch size for parallelization of training.\n",
    "        test_size: A decimal value for train/valid split used for the cross\n",
    "            validation splitting.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        A tuple containing two lists, each the training and validation loss\n",
    "        respectively, over all epochs of training.\n",
    "\n",
    "    \"\"\"\n",
    "    N = X_train.shape[0]\n",
    "    N_train = math.ceil((N * (1 - test_size)) / bs)\n",
    "    N_test = math.ceil((N * test_size) / bs)\n",
    "    print('Starting training.')\n",
    "    print(f'Number of minibatches for training/test: {N_train}/{N_test}')\n",
    "    t_losses = []\n",
    "    v_losses = []\n",
    "    for e in range(epochs):\n",
    "        print(f'Starting epoch {e+1} of {epochs}')\n",
    "        cv_splits = cv_sets(X_train, k, 0.2, None)\n",
    "\n",
    "        starttime = datetime.now()\n",
    "        batch_tl = [] # batch train losses\n",
    "        batch_vl = []\n",
    "        for i, (train_ix, valid_ix) in enumerate(cv_splits):\n",
    "            train_batches = batcherize(X_train, y_train, train_ix, bs)\n",
    "            valid_batches = batcherize(X_train, y_train, valid_ix, bs)\n",
    "            model.train()\n",
    "            for inputs, targets in train_batches:\n",
    "                optim.zero_grad()\n",
    "                out = model(inputs)\n",
    "                loss = loss_fn(out, targets)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                batch_tl.append(loss.item())\n",
    "            model.eval()\n",
    "            for inputs, targets in valid_batches:\n",
    "                out = model(inputs)\n",
    "                loss = loss_fn(out, targets)\n",
    "                batch_vl.append(loss.item())\n",
    "\n",
    "        t_loss = np.mean(batch_tl)\n",
    "        t_losses.append(t_loss)\n",
    "        v_losses.append(np.mean(batch_vl))\n",
    "        secs_elapsed = (datetime.now() - starttime).total_seconds()\n",
    "        print(f'\\tTraining took: {secs_elapsed:0.2f}s\\twith loss: {t_loss:0.4f}')\n",
    "    return t_losses, v_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 1, 256, 256]), tensor([320, 232,  76]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 1000\n",
    "x_rand = torch.randn((N, 1, 256, 256))\n",
    "y_rand = torch.randint(512, (N,))\n",
    "x_rand.shape, y_rand[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n",
      "Number of minibatches for training/test: 50/13\n",
      "Starting epoch 1 of 3\n",
      "\tTraining took: 725.01s\twith loss: 6.2407\n",
      "Starting epoch 2 of 3\n",
      "\tTraining took: 1078.54s\twith loss: 6.2408\n",
      "Starting epoch 3 of 3\n",
      "\tTraining took: 789.74s\twith loss: 6.2409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([6.240675853729248, 6.240800853729248, 6.240925853729248],\n",
       " [6.240714315267709, 6.24023354603694, 6.239752776806171])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, 3, x_rand, y_rand, optim, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
