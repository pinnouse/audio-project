{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = os.path.join(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('english\\\\Durian',\n",
       " [],\n",
       " ['aligned.swc',\n",
       "  'audio.ogg',\n",
       "  'audiometa.txt',\n",
       "  'info.json',\n",
       "  'wiki.html',\n",
       "  'wiki.txt',\n",
       "  'wiki.xml'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(os.walk(os.path.join(DATA_DIR, 'Durian')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PronunciationTiming(object):\n",
    "\n",
    "    def __init__(self, word: str, start: int, end: int) -> None:\n",
    "        self.word = word\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "class SWC(object):\n",
    "\n",
    "    def __init__(self, swc_path: str) -> None:\n",
    "        self.tree = ET.parse(swc_path)\n",
    "        self.timings = []\n",
    "\n",
    "    def get_timings(self) -> List[PronunciationTiming]:\n",
    "        if len(self.timings) > 0:\n",
    "            return self.timings\n",
    "        # https://www.geeksforgeeks.org/xml-parsing-python/\n",
    "        root = self.tree.getroot()\n",
    "        doc = root.find('d')\n",
    "\n",
    "        for p in doc.findall('p'):\n",
    "            for s in p.findall('s'):\n",
    "                for t in s.findall('t'):\n",
    "                    for n in t.findall('n'):\n",
    "                        attribs = n.attrib\n",
    "                        if 'pronunciation' not in attribs \\\n",
    "                            or 'start' not in attribs or 'end' not in attribs:\n",
    "                            continue\n",
    "                        word = attribs['pronunciation'].lower()\n",
    "                        self.timings.append(PronunciationTiming(\n",
    "                            word,\n",
    "                            int(attribs['start']),\n",
    "                            int(attribs['end'])))\n",
    "        return self.timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "def split_clip(audio: AudioSegment, swc: SWC\n",
    "               ) -> Tuple[List[str], List[AudioSegment]]:\n",
    "    words = []\n",
    "    clips = []\n",
    "    for p in swc.get_timings():\n",
    "        words.append(p.word)\n",
    "        clip = audio[p.start:p.end]\n",
    "        # https://github.com/jiaaro/pydub/blob/master/API.markdown#audiosegmentget_array_of_samples\n",
    "        # clip = audio[p.start:p.end].get_array_of_samples()\n",
    "        # fp_arr = np.array(clip).T.astype(np.float32)\n",
    "        # fp_arr /= np.iinfo(clip.typecode).max\n",
    "        clips.append(clip)\n",
    "    return words, clips\n",
    "\n",
    "# audio = AudioSegment.from_ogg('english/Durian/audio.ogg')\n",
    "# print(audio.channels, audio.frame_rate)\n",
    "# swc = SWC('english/Durian/aligned.swc')\n",
    "# split_clip(audio, swc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "class PronunciationClip(object):\n",
    "    def __init__(self, word: str, audio: AudioSegment) -> None:\n",
    "        self.word = word\n",
    "        self.audio = audio\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"PronunciationClip({repr(self.word)}, {repr(self.audio)})\"\n",
    "\n",
    "def load_utterances(limit: Optional[int] = 5000, vocab: List[str] = None\n",
    "                    ) -> List[PronunciationClip]:\n",
    "    clips = []\n",
    "\n",
    "    for dir in os.listdir(DATA_DIR):\n",
    "        path = os.path.join(DATA_DIR, dir)\n",
    "        if not os.path.isdir(path):\n",
    "            continue\n",
    "        files = os.listdir(path)\n",
    "        if 'audio.ogg' not in files or 'aligned.swc' not in files:\n",
    "            continue\n",
    "        try:\n",
    "            audio = AudioSegment.from_ogg(os.path.join(path, 'audio.ogg'))\n",
    "        except:  # noqa: E722\n",
    "            print(f'Failed to decode file: {os.path.join(path, \"audio.ogg\")}')\n",
    "            continue\n",
    "        swc = SWC(os.path.join(path, 'aligned.swc'))\n",
    "        words, pcms = split_clip(audio, swc)\n",
    "        for w, c in zip(words, pcms):\n",
    "            # skip words less than 4 chars long or not in vocab\n",
    "            if len(w) < 4 \\\n",
    "                or vocab is not None and w not in vocab:\n",
    "                continue\n",
    "            clips.append(PronunciationClip(w, c))\n",
    "\n",
    "        if limit is not None and len(clips) > limit:\n",
    "            break\n",
    "    return clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to decode file: english\\Caesium\\audio.ogg\n",
      "Failed to decode file: english\\Fuck_(film)\\audio.ogg\n",
      "Failed to decode file: english\\Spinning_Around\\audio.ogg\n",
      "100283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PronunciationClip('cant', <pydub.audio_segment.AudioSegment object at 0x0000017DCCD26180>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clips = load_utterances(None) # 50 minutes to load 100,000 clips\n",
    "# print(len(clips))\n",
    "# clips[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noise_playlist(path: os.PathLike, limit: int = 0) -> List[AudioSegment]:\n",
    "    clips = []\n",
    "    for f in os.listdir(path):\n",
    "        if not f.endswith('wav'):\n",
    "            continue\n",
    "        file_path = os.path.join(path, f)\n",
    "        clips.append(AudioSegment.from_file(file_path))\n",
    "        if limit > 0 and len(clips) > limit:\n",
    "            break\n",
    "    return clips\n",
    "\n",
    "\n",
    "# music_clips = make_noise_playlist(os.path.join('audio', 'music'), 200)\n",
    "# outdoor_clips = make_noise_playlist(os.path.join('audio', 'outside_rural'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cupof\\repos\\audio-project\\.venv\\Lib\\site-packages\\pycochleagram-0.1-py3.12.egg\\pycochleagram\\erbfilter.py:8: RuntimeWarning: pycochleagram using non-interactive Agg matplotlib backend\n",
      "  from pycochleagram import utils\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/mcdermottLab/pycochleagram\n",
    "import pycochleagram.cochleagram as cgram\n",
    "from scipy import ndimage\n",
    "\n",
    "N_FILTERS = 50\n",
    "LO_LIM = 30\n",
    "HI_LIM = 7860\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "def gen_clip(audio: AudioSegment, duration: int = 2000) -> AudioSegment:\n",
    "    clip = AudioSegment.silent(duration=duration, frame_rate=SAMPLE_RATE)\n",
    "    audio_duration = len(audio)\n",
    "    offset = max(0, duration / 2 - (audio_duration / 2))\n",
    "    return clip.overlay(audio, offset)\n",
    "\n",
    "def audio_to_cgram(audio: AudioSegment) -> np.array:\n",
    "    # TODO: find audio input type for cochleagram (8-bit integer PCM?)\n",
    "    arr = np.array(audio.get_array_of_samples())\n",
    "    return cgram.human_cochleagram(arr, audio.frame_rate or SAMPLE_RATE,\n",
    "                                   N_FILTERS, LO_LIM, HI_LIM, 4, downsample=200)\n",
    "\n",
    "def aa_cgram(cochleagram: np.ndarray, size: int = 256) -> np.ndarray:\n",
    "    f, t = cochleagram.shape\n",
    "    kx = size / f\n",
    "    ky = size / t\n",
    "    return ndimage.zoom(cochleagram, (kx, ky))\n",
    "\n",
    "# a = gen_clip(clips[0].audio)\n",
    "# c = audio_to_cgram(a)\n",
    "# aa = aa_cgram(c)\n",
    "# c.shape, aa.shape # 211x400, 256x256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import array\n",
    "\n",
    "\n",
    "def overlay_noise(snr: float, signal: AudioSegment, noise: AudioSegment\n",
    "                  ) -> AudioSegment:\n",
    "    # signal = signal.normalize()\n",
    "    # noise = noise.normalize()\n",
    "    # gain = snr - signal.dBFS + noise.dBFS\n",
    "    # scaled_noise = noise.apply_gain(-gain)\n",
    "    # print(np.min(s), np.max(s))\n",
    "    # print(np.min(ss), np.max(ss))\n",
    "    signal_rms = signal.rms\n",
    "    noise_rms = noise.rms\n",
    "    scaling_factor = signal_rms / noise_rms * 10**(-snr/20)\n",
    "    noise_np = np.array(noise.set_channels(1).get_array_of_samples())\n",
    "    noise_samples = noise_np * scaling_factor\n",
    "    # new_noise_rms = np.sqrt(np.mean(np.square(noise_samples)))\n",
    "    # new_snr = (signal_rms / new_noise_rms) ** 2\n",
    "    noise_array = array.array(signal.array_type, np.round(noise_samples).astype(np.int32))\n",
    "    scaled_noise = noise._spawn(noise_array)\n",
    "    return signal.overlay(scaled_noise, loop=True)\n",
    "\n",
    "# noise = AudioSegment.from_file('audio/outside_rural/snipped119_start_240_end_250.wav')\n",
    "# overlay_noise(2.0, gen_clip(clips[0].audio), noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count word frequencies to consider which data points we will keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x17d81714260>,\n",
       "  <matplotlib.lines.Line2D at 0x17d81717ec0>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x17d81717c80>,\n",
       "  <matplotlib.lines.Line2D at 0x17d81717980>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x17d81714560>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x17d817176b0>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x17d81717440>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAs2UlEQVR4nO3df3RU9Z3/8ddMfgnJBOIPJgEFg0GpBukmYBLrGCFSZI2VWV3FetTualsspx7PguxGd/eI31OTLWdxdyt6TherrSK7blpC1x8gUBNSmqWCRcOqrJMA9YTJRBJCQhLJj/l8/7AZM5LSIJPMvTfPxznvc2bu553MJzmH3Bf33s+9LklGAAAANuKO9wQAAADOFgEGAADYDgEGAADYDgEGAADYDgEGAADYDgEGAADYDgEGAADYDgEGAADYTmK8JzCapk6dqs7OznhPAwAAnAWPx6OjR4+escexAWbq1KlqamqK9zQAAMCXMG3atDOGGMcGmMEjL9OmTeMoDAAANuHxeNTU1PQn992ODTCDOjs7CTAAADjMWV/E6/P59Mtf/lJNTU0yxujWW289rWfNmjU6evSouru7tX37duXk5ESNZ2Rk6KWXXtKJEyd0/PhxbdiwQampqVE9c+bM0a5du9TT06Pf//73euSRR852qgAAwKHOOsCkpqbq3Xff1YoVK4YdX716tR566CEtX75cBQUF6urq0rZt25SSkhLp2bhxo6666iotWrRIpaWluv766/XjH/84Mu7xePTmm2/qyJEjys/P1yOPPKLHH39c3/72t7/EjwgAAJzIfNkyxphbb701atvRo0fNypUrI+/T09NNT0+PufPOO40kM3v2bGOMMfn5+ZGexYsXm4GBAZOVlWUkmeXLl5vW1laTlJQU6SkvLzcffPDBiOfm8XiMMcZ4PJ4v/fNRFEVRFDW2NdL9d0zvA5Odna2srCzt2LEjsq2jo0N79uxRUVGRJKmoqEjHjx/Xvn37Ij07duxQOBxWQUFBpGfXrl3q6+uL9Gzbtk2zZ8/W5MmTh/3s5ORkeTyeqAIAAM4U0wCTmZkpSQqFQlHbQ6FQZCwzM1MtLS1R4wMDA2pra4vqGe57DP2MLyorK1NHR0ekWEINAIBzOeZOvOXl5UpPT4/UtGnT4j0lAAAwSmIaYJqbmyVJXq83arvX642MNTc3a8qUKVHjCQkJOv/886N6hvseQz/ji3p7eyNLplk6DQCAs8U0wBw6dEjBYFAlJSWRbR6PRwUFBaqrq5Mk1dXVKSMjQ3l5eZGehQsXyu12a8+ePZGe66+/XomJn9+mZtGiRfrwww/V3t4eyykDAACbOqurg1NTU83cuXPN3LlzjTHGPPzww2bu3LnmkksuMZLM6tWrTVtbm7nllltMbm6u2bx5s2loaDApKSmR7/H666+bffv2mfnz55trr73WHDx40GzcuDEynp6eboLBoPnpT39qrrzySnPHHXeYkydPmm9/+9sxv4qZoih7ldvtNsXFxWbZsmWmuLjYuN3uuM+JoqjY1Vnsv8/uGxcXF5vhPP/885GeNWvWmGAwaHp6esz27dvNrFmzor5HRkaG2bhxo+no6DDt7e3mueeeM6mpqVE9c+bMMbt27TI9PT3m448/NqtXrx6tXwBFUTYpv99vGhsbo/72NDY2Gr/fH/e5URQVmxq1AGOXIsBQlLPK7/ebgYEBs2XLFlNQUGBSU1NNQUGB2bJlixkYGCDEUJRDaqT7b9cfXjiOx+NRR0eH0tPTuaAXsDm3261AIKD6+notXbpUxnz+Z8vlcqmqqkq5ubmaNWuWwuFwHGcK4FyNdP/tmGXUAJzL5/MpOztbTz75ZFR4kSRjjMrLyzVz5kz5fL44zRDAWCPAALC8rKwsSdKBAweGHR/cPtgHwPkIMAAsLxgMSpJyc3OHHR/cPtgHwPkIMAAsr7a2VocOHdKjjz4ql8sVNeZyuVRWVqbGxkbV1tbGaYYAxhoBBoDlhcNhrVy5UqWlpaqqqlJhYaHS0tJUWFioqqoqlZaWatWqVVzAC4wzcV8yNRrFMmqKcl4Ndx+YhoYGllBTlIOKZdQsowYcye12y+fzKSsrS8FgULW1tRx5ARxkpPvvxD86AgAWFA6HVVNTE+9pAIgzroEBAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2Q4ABAAC2kxjvCQDA2XC73fL5fMrKylIwGFRtba3C4XC8pwVgjHEEBoBt+P1+BQIBVVdXa9OmTaqurlYgEJDf74/31ACMMQIMAFvw+/2qrKxUfX29CgsLlZaWpsLCQtXX16uyspIQA4wzLkkm3pMYDR6PRx0dHUpPT1dnZ2e8pwPgHLjdbgUCAdXX12vp0qUy5vM/Wy6XS1VVVcrNzdWsWbM4nQTY3Ej33xyBAWB5Pp9P2dnZevLJJ6PCiyQZY1ReXq6ZM2fK5/PFaYYAxhoBBoDlZWVlSZIOHDgw7Pjg9sE+AM5HgAFgecFgUJKUm5s77Pjg9sE+AM5HgAFgebW1tTp06JAeffRRuVyuqDGXy6WysjI1NjaqtrY2TjMEMNYIMAAsLxwOa+XKlSotLVVVVVXUKqSqqiqVlpZq1apVXMALjDPGieXxeIwxxng8nrjPhaKo2JTf7zeNjY1mqIaGBuP3++M+N4qiYlMj3X+zjBqArXAnXsDZRrr/5lECAGwlHA6rpqYm3tMAEGdcAwMAAGyHAAMAAGyHAAMAAGyHAAMAAGyHAAMAAGwn5gHG7XbriSeeUGNjo7q7uxUIBPT3f//3p/WtWbNGR48eVXd3t7Zv366cnJyo8YyMDL300ks6ceKEjh8/rg0bNig1NTXW0wUAADYV0xvQlJWVmU8++cT8+Z//uZkxY4a57bbbTEdHh/n+978f6Vm9erU5fvy4+cY3vmHmzJljqqqqTENDg0lJSYn0vP766+Z3v/udueaaa8zXvvY183//939m48aNMb8RDkVRFEVR1qmz2H/H9oP/+7//22zYsCFqW2VlpXnxxRcj748ePWpWrlwZeZ+enm56enrMnXfeaSSZ2bNnG2OMyc/Pj/QsXrzYDAwMmKysrFj/AiiKoiiKskiNdP8d81NIv/nNb1RSUqJZs2ZJkq6++mpdd911euONNyRJ2dnZysrK0o4dOyJf09HRoT179qioqEiSVFRUpOPHj2vfvn2Rnh07digcDqugoGDYz01OTpbH44kqAADgTDG/E29FRYXS09P14YcfamBgQAkJCXrsscf08ssvS5IyMzMlSaFQKOrrQqFQZCwzM1MtLS1R4wMDA2pra4v0fFFZWZkef/zxGP80AADAimJ+BOaOO+7Q3XffrW9+85vKy8vTfffdp1WrVunee++N9UdFKS8vV3p6eqSmTZs2qp8HAADiJ+ZHYNauXauKigr953/+pyTpwIEDmjFjhsrKyvSzn/1Mzc3NkiSv1xt5Pfh+//79kqTm5mZNmTIl6vsmJCTo/PPPj/qaoXp7e9Xb2xvrHwcAAFhQzI/ATJw48bQnww4MDMjt/uyjDh06pGAwqJKSksi4x+NRQUGB6urqJEl1dXXKyMhQXl5epGfhwoVyu93as2dPrKcMAABsKKZXDz///PPm448/jiyjXrp0qWlpaTEVFRWRntWrV5u2tjZzyy23mNzcXLN58+Zhl1Hv27fPzJ8/31x77bXm4MGDLKOmKIqiKIdX3JZRp6WlmaeeesocPnzYdHd3m0AgYP7f//t/JikpKapvzZo1JhgMmp6eHrN9+3Yza9asqPGMjAyzceNG09HRYdrb281zzz1nUlNTR+MXQFEURVGURWqk+2/XH144jsfjUUdHh9LT09XZ2Rnv6QCIEbfbLZ/Pp6ysLAWDQdXW1p522hqAfY10/82zkADYht/vVyAQUHV1tTZt2qTq6moFAgH5/f54Tw3AGCPAALAFv9+vyspK1dfXq7CwUGlpaSosLFR9fb0qKysJMcA4wykkAJbndrsVCARUX1+vpUuXypjP/2y5XC5VVVUpNzdXs2bN4nQSYHOcQgLgGD6fT9nZ2XryySejwoskGWNUXl6umTNnyufzxWmGAMYaAQaA5WVlZUn67MaYwxncPtgHwPkIMAAsLxgMSpJyc3OHHR/cPtgHwPkIMAAsr7a2VocOHdKjjz4ql8sVNeZyuVRWVqbGxkbV1tbGaYYAxhoBBoDlhcNhrVy5UqWlpaqqqopahVRVVaXS0lKtWrWKC3iBcSbud90bjeJOvBTlvPL7/aaxsdEM1dDQYPx+f9znRlFUbIo78bKMGnAk7sQLONtI99+JYzgnADhn4XBYNTU18Z4GgDjjGhgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7BBgAAGA7oxJgpk6dqhdffFHHjh1Td3e33nvvPeXn50f1rFmzRkePHlV3d7e2b9+unJycqPGMjAy99NJLOnHihI4fP64NGzYoNTV1NKYLAABsJuYBZvLkydq9e7f6+vq0ZMkSXXnllVq5cqWOHz8e6Vm9erUeeughLV++XAUFBerq6tK2bduUkpIS6dm4caOuuuoqLVq0SKWlpbr++uv14x//ONbTBQAANmViWeXl5WbXrl1n7Dl69KhZuXJl5H16errp6ekxd955p5FkZs+ebYwxJj8/P9KzePFiMzAwYLKyskY0D4/HY4wxxuPxxPTnoyiKoihq9Gqk+++YH4H5xje+ob179+qVV15RKBTSO++8owceeCAynp2draysLO3YsSOyraOjQ3v27FFRUZEkqaioSMePH9e+ffsiPTt27FA4HFZBQcGwn5ucnCyPxxNVAADAmWIeYGbOnKkHH3xQH330kRYvXqxnn31W//Zv/6Z7771XkpSZmSlJCoVCUV8XCoUiY5mZmWppaYkaHxgYUFtbW6Tni8rKytTR0RGppqamWP9oAADAIhJj/Q3dbrf27t2rxx57TJK0f/9+5ebmavny5frZz34W64+LKC8v17p16yLvPR4PIQZwILfbLZ/Pp6ysLAWDQdXW1iocDsd7WgDGWMyPwASDQb3//vtR2z744ANNnz5dktTc3CxJ8nq9UT1erzcy1tzcrClTpkSNJyQk6Pzzz4/0fFFvb686OzujCoCz+P1+BQIBVVdXa9OmTaqurlYgEJDf74/31ACMsZgHmN27d+uKK66I2nb55ZfryJEjkqRDhw4pGAyqpKQkMu7xeFRQUKC6ujpJUl1dnTIyMpSXlxfpWbhwodxut/bs2RPrKQOwAb/fr8rKStXX16uwsFBpaWkqLCxUfX29KisrCTHAOBTTq4fnzZtnent7TVlZmbnsssvMXXfdZU6ePGm++c1vRnpWr15t2trazC233GJyc3PN5s2bTUNDg0lJSYn0vP7662bfvn1m/vz55tprrzUHDx40GzdujPlVzBRFWb/cbrdpbGw0W7ZsMS6XK2rM5XKZLVu2mIaGBuN2u+M+V4qizq3OYv8d+w+/+eabzXvvvWd6enrM+++/bx544IHTetasWWOCwaDp6ekx27dvN7NmzYoaz8jIMBs3bjQdHR2mvb3dPPfccyY1NXU0fgEURVm8iouLjTHGFBQUDDteWFhojDGmuLg47nOlKOrcKq4BxgpFgKEo59SyZcuMMeaP/icmLS3NGGPMsmXL4j5XiqLOreJ2HxgAiLVgMChJys3NHXZ8cPtgHwDnI8AAsLza2lodOnRIjz76qFwuV9SYy+VSWVmZGhsbVVtbG6cZAhhrBBgAlhcOh7Vy5UqVlpaqqqoqahVSVVWVSktLtWrVKu4HA4wzcT/fNRrFNTAU5bzy+/2msbHRDNXQ0GD8fn/c50ZRVGxqpPtv1x9eOI7H41FHR4fS09O5qR3gIMnJyfrnf/5n5eTkKBAIaOXKlert7Y33tADEyNnsv+OetkajOAJDUc6riooK09vbG3UEpre311RUVMR9bhRFxaZYhQTAUSoqKrR69Wq1trbq/vvvl9fr1f3336/W1latXr1aFRUV8Z4igDHEKSQAlpeYmKju7m61trZq2rRpURfrut1uNTU16YILLtDEiRPV398fx5kCOFcj3X9zBAaA5a1YsUJJSUl67LHHTltpFA6H9Q//8A9KSkrSihUr4jRDAGONAAPA8i677DJJ0quvvjrs+OD2wT4AzkeAAWB5DQ0NkqTS0tJhxwe3D/YBcD6ugQFgeVwDA4wfXAMDwDH6+/u1bt06eb1eNTU16YEHHlBmZqYeeOABNTU1yev1at26dYQXYJyJ+5rv0SjuA0NRzivuA0NRzi/uxMspJMCREhMTtWLFCl122WVqaGjQ+vXrOfICOMhI998EGAAAYBlcAwMAAByLAAMAAGyHAAMAAGyHAAMAAGyHAAMAAGyHAAMAAGyHAAMAAGyHAAMAAGyHAAMAAGyHAAMAAGyHAAMAAGyHAAMAAGyHAAMAAGyHAAMAAGyHAAMAAGyHAAMAAGyHAAMAAGyHAAMAAGyHAAMAAGyHAAMAAGwnMd4TAICz4Xa75fP5lJWVpWAwqNraWoXD4XhPC8AY4wgMANvw+/0KBAKqrq7Wpk2bVF1drUAgIL/fH++pARhjHIEBYAt+v1+VlZV67bXXVFVVpQkTJqinp0c5OTmqrKzU7bffrs2bN8d7mgDGiEuSifckRoPH41FHR4fS09PV2dkZ7+kAOAdut1uBQED9/f269NJLlZSUFBnr6+vT4cOHlZCQoFmzZnE6CbC5ke6/OYUEwPJ8Pp+ys7OVk5Oj1tZW3X///fJ6vbr//vvV2tqqnJwczZw5Uz6fL95TBTBGOIUEwPIuvvhiSVJLS4umTZsWOcryk5/8RC+88IKOHj0qr9cb6QPgfByBAWB5hYWFkqQNGzacdoooHA7rJz/5SVQfAOcjwACwjXnz5snlckVtc7lcysvLi9OMAMQLAQaA5QUCAUnSokWLVFVVpcLCQqWlpamwsFBVVVVatGhRVB8A52MVEgDLS0xMVHd3t06ePKn29nZlZ2dHxhobG5WRkaG0tDRNnDhR/f39cZwpgHPFKiQAjtHf369169Zp8uTJmjBhgtauXasHH3xQa9eu1cSJEzV58mStW7eO8AKMM8aJ5fF4jDHGeDyeuM+FoqjYVEVFhent7TVD9fb2moqKirjPjaKo2NRI99+cQgJgK4mJiVqxYoUuu+wyNTQ0aP369Rx5ARxkpPtv7gMDwFbC4bD279+vUCikYDDInXeBcYprYADYBg9zBDCIAAPAFgYf5jhlypSo7VOmTFFlZSUhBhhnCDAALM/tduvZZ5+Vy+XSzp07o+4Ds3PnTrlcLj3zzDNyu/mTBowncb/ieDSKVUgU5ZxasGCBMcaYXbt2GZfLFTXmcrnMrl27jDHGLFiwIO5zpSjq3Gqk+2/+uwLA8m644QZJ0j/+4z/KGBM1ZozR448/HtUHwPkIMAAAwHYIMAAsr7q6WpL0xBNPDPswx8EjMIN9AJyPAAPA8mpqahQKhXTdddcN+zDH6667Ts3NzaqpqYn3VAGMoVG9GOdv//ZvjTHGPPXUU5FtKSkp5umnnzbHjh0znZ2dprKy0kyZMiXq6y655BLz6quvmq6uLhMKhcwPf/hDk5CQEPOLgCiKskf5/X4zMDBgTp48GfUogZMnT5qBgQHj9/vjPkeKos69zmL/PXqTmDdvnmlsbDT79++PCjDPPPOMOXLkiFmwYIHJy8szv/nNb8yvf/3ryLjb7TbvvfeeefPNN83cuXPNTTfdZFpaWswPfvCD0fgFUBRlk/L7/aaxsTEqwDQ0NBBeKMpBFfcAk5qaag4ePGhKSkrMW2+9FQkw6enp5tSpU+a2226L9F5xxRXGGGMKCgqMJHPTTTeZ/v7+qKMy3/3ud017e7tJSkqK9S+AoigbldvtNsXFxWbZsmWmuLjYuN3uuM+JoqjYVdyXUa9fv16vvfaadu7cGbU9Pz9fycnJ2rFjR2TbwYMHdeTIERUVFUmSioqKVF9fr5aWlkjPtm3bNGnSJF111VXDfl5ycrI8Hk9UAXCecDismpoa/cd//Idqamp4FhIwTo3KwxzvvPNO5eXlaf78+aeNZWZm6tSpUzpx4kTU9lAopMzMzEhPKBQ6bXxwbDhlZWWRlQgAAMDZYn4E5uKLL9a//uu/6u6779apU6di/e3/qPLycqWnp0dq2rRpY/bZAABgbMU8wOTn58vr9eqdd95RX1+f+vr6dMMNN+ihhx5SX1+fQqGQUlJSNGnSpKiv83q9am5uliQ1NzfL6/WeNj44Npze3l51dnZGFQAAcKaYB5idO3cqNzdXX/3qVyP19ttva+PGjfrqV7+qvXv3qre3VyUlJZGvufzyyzVjxgzV1dVJkurq6jRnzhxddNFFkZ5FixbpxIkTev/992M9ZQA24na7VVxcrGXLlqm4uJgHOALj2KhfUTx0FZL02TLqw4cPmxtuuMHk5eWZ3bt3m927d39+ZfEfllFv3brVXH311ebrX/+6CYVCLKOmqHFewy2jbmxsZBk1RTmo4r6Memh9McAM3siutbXVnDx50vz85z83Xq836mumT59uXnvtNdPV1WVaWlrM2rVruZEdRY3jGryR3ZYtW0xBQYFJTU01BQUFZsuWLdzIjqIcVCPdf7v+8MJxPB6POjo6lJ6ezvUwgM253W4FAgHV19dr6dKlUU+kdrlcqqqqUm5urmbNmsWyasDmRrr/5uQxAMvz+XzKzs7Wk08+GRVeJMkYo/Lycs2cOVM+ny9OMwQw1ggwACwvKytLknTgwIFhxwe3D/YBcD4CDADLCwaDkqTc3Nxhxwe3D/YBcD4CDADLq62t1aFDh/Too4/K5XJFjblcLpWVlamxsVG1tbVxmiGAsUaAAWB54XBYK1euVGlpqaqqqlRYWKi0tDQVFhaqqqpKpaWlWrVqFRfwAuNM3JdMjUaxjJqinFfD3QemoaGBJdQU5aBiGTXLqAFHcrvd8vl8ysrKUjAYVG1tLUdeAAcZ6f57VJ5GDQCjJRwOq6amJt7TABBnXAMDAABshwADAABsh1NIAGyFa2AASByBAWAjfr9fgUBA1dXV2rRpk6qrqxUIBOT3++M9NQBjjCMwAGzB7/ersrJSr776qtauXauenh5NmDBBN910kyorK3X77bdr8+bN8Z4mgDHCMmoAljf4NOpjx47pwgsvVHZ2dmTs0KFDOnbsmC644AKeRg04AE+jBuAYg0+jzs/PV319fdSdeOvr65Wfn8/TqIFxhgADwPKmTp0qSXrjjTe0dOlS7dmzR11dXdqzZ4+WLl2qrVu3RvUBcD4CDADLmzJliiTpF7/4hYyJPuttjIlc+zLYB8D5CDAALO+TTz6RJP3FX/zFsE+jXrp0aVQfAOdjFRIAy2tqapIkLVmyRFu2bNEbb7wRWYW0ZMkSLVmyJKoPgPOxCgmA5Q2uQurv79ell16qpKSkyFhfX58OHz6shIQEViEBDsAqJACOEQ6H9corrygnJ0etra1au3atHnzwQa1du1atra3KycnRf/3XfxFegHGEIzAALO9M94FpbGxUa2sr94EBHGKk+2+ugQFgeYP3gbnrrrv09ttvn/YspGuuuUZ1dXXy+XyqqamJ93QBjAECDADLy8rKkiQdOHBA4XD4tJBy4MCBqD4Azsc1MAAsLxgMSpJyc3PldrtVXFysZcuWqbi4WG63W7m5uVF9AJyPa2AAWB7PQgLGD1YhAXCMwVVI8+bN0yWXXBI1dskll2jevHmsQgLGGQIMAMtzu9361re+FXn9xTFJuu+++04bA+Bc/GsHYHnFxcXyer2SpJ6enqixwfeZmZkqLi4e87kBiA8CDADLW7BgQeT1zp07VVhYqLS0NBUWFmrnzp3D9gFwNpZRA7C8GTNmSJLq6+u1dOnSyBOp9+zZo6VLl2r//v26+uqrI30AnI8jMAAsb/AJ1IPB5U/1AXA+AgwAyzt8+LAk6eqrr1ZVVVXUKaSqqirNmTMnqg+A8xFgAFjeW2+9FXldUlKiuro6dXZ2qq6uTiUlJcP2AXA2AgwAy6upqVEoFJL0x08TNTc38xwkYBwhwACwvHA4rAcffFDGGE2YMCFqbOLEiTLG6Hvf+x43sgPGEQIMAFsoKCiQy+XSwMBA1PaBgQG5XC4VFBTEaWYA4oFnIQGwvMTERHV3d+vkyZNqb28/7VlIkydPVlpamiZOnKj+/v44zhTAueJZSAAcY8WKFUpKStKkSZM0ZcqUqLEpU6Zo0qRJSkpK0ooVK+I0QwBjjQADwPJycnIkfXYB73B34h28sHewD4DzEWAAWN5gQPnoo490++23q7CwUOXl5SosLNTtt9+uQCAQ1QfA+XiUAADLa29vlyRdfPHF6urqUlJSUmRs7dq16uvri+oD4HwEGACWN7jy6ItLqKXPLvBNTEyM6gPgfJxCAmB5tbW1kddffB7S0PdD+wA4GwEGgOUNPutIkrZu3apAIKBjx44pEAho69atw/YBcDYCDADLu+666yKvlyxZopycHF144YXKycnRkiVLhu0D4GwEGACWd/LkyZj2AbA/LuIFYHmbNm3SvffeK0l69dVX9emnn2ry5Mlqb2/Xeeedp9LS0kgfgPGBAAPA8r7zne9EXpeWlkbd72XoRbzf+c53oq6JAeBcnEICYHkjvTiXi3iB8YMAA8Dyjh8/Hnnd3d0dNTb0/dA+AM7GKSQAlrdv3z7Nnz9fknTBBReosLBQWVlZCgaD+p//+R/19PRE+gCMDxyBAWB5Q59A3dXVpbKyMk2fPl1lZWXq6uoatg+As3EEBoDldXZ2Svrsgl23263Fixdr8eLFkW3GGLlcrkgfAOfjCAwAy3vxxRclffa06eEeJTC4KmmwD4DzEWAAWF5NTc2ffFDjwMCAampqxmhGAOKNAAPA8nw+nxISEiQp6h4wQ98nJCTI5/ON+dwAxAcBBoDlLViwQJLU1tY27Pjg9sE+AM5HgAFgedOnT5cknX/++cOOD24f7APgfDEPMH/3d3+n3/72t+ro6FAoFNLmzZt1+eWXR/WkpKTo6aef1rFjx9TZ2anKysrTlj9ecsklevXVV9XV1aVQKKQf/vCHkUPIAMaXpqamyOsvXgsz9P3QPgDOFvMAU1xcrPXr16uwsFCLFi1SUlKS3nzzTU2cODHS89RTT+mWW27RX/7lX6q4uFhTp07VL37xi88n5XbrtddeU3Jysq699lrdd999+ta3vqUnnngi1tMFYAMZGRmR18eOHdOmTZu0YcMGbdq0SceOHRu2D4DzmdGsCy+80BhjjM/nM5JMenq6OXXqlLntttsiPVdccYUxxpiCggIjydx0002mv7/fTJkyJdLz3e9+17S3t5ukpKQRfa7H4zHGGOPxeEb156MoavSrurraGGNMOBw24XDYDDV0W3V1ddznSlHUudVI99+jfg3MpEmTJH1+kV1+fr6Sk5O1Y8eOSM/Bgwd15MgRFRUVSZKKiopUX1+vlpaWSM+2bds0adIkXXXVVcN+TnJysjweT1QBcIbJkydHXg93H5jh+gA426gGGJfLpX/5l3/Rr3/9a/3v//6vJCkzM1OnTp3SiRMnonpDoZAyMzMjPaFQ6LTxwbHhlJWVqaOjI1KcCwec48CBA5I+Cyu///3vo8aOHDkSCTGDfQCcb1QfJbB+/Xrl5ubquuuuG82PkSSVl5dr3bp1kfcej4cQAzjE4H94XC6XzjvvPK1du1aNjY2aOXOm7rnnnsi9YL74HyMAzjVqAeZHP/qRSktLdf3110cFiebmZqWkpGjSpElRf2y8Xq+am5sjPddcc03U9/N6vZGx4fT29qq3tzfWPwYACxi60sjr9eqRRx6JvB96CulP3a0XgHOMyimkH/3oR/L7/Vq4cKEOHz4cNbZv3z719vaqpKQksu3yyy/XjBkzVFdXJ0mqq6vTnDlzdNFFF0V6Fi1apBMnTuj9998fjSkDsLCGhobI6zNdAzO0D4DzxfTq4fXr15vjx4+b66+/3ni93kidd955kZ5nnnnGHD582Nxwww0mLy/P7N692+zevfvzK4vdbvPee++ZrVu3mquvvtp8/etfN6FQyPzgBz+I+VXMFEVZvxITE01vb69pa2szhw4dilqF1NjYaNra2kxvb69JTEyM+1wpijq3Oov9d2w/+I+57777Ij0pKSnm6aefNq2trebkyZPm5z//ufF6vVHfZ/r06ea1114zXV1dpqWlxaxdu9YkJCSMxi+AoigbVEVFhQmHw6avry/qb0tfX58Jh8OmoqIi7nOkKOrcK24BxipFgKEoZ5Xf7z/jfWD8fn/c50hR1LmXZe4DAwDnyu1269lnn5Uk9fT0RI0Nvn/mmWfkdvMnDRgvRnUZNQDEQnFxsbxer4wx+tWvfqWenh5NnjxZ7e3tmjBhgm6++WZlZmaquLhYb731VrynC2AMEGAAWN6CBQskfXZH75tvvjly3xdJMsaora1NF1xwgRYsWECAAcYJjrcCsLzp06dLks4///xhxwe3D/YBcD4CDADLCwaDkddnug/M0D4AzsYpJACW95WvfCXyuqWlRW+99Za6urqUmpqqBQsWRO7UPbQPgLMRYABY3ty5cyOvvV6v7rrrrsj7oUdghvYBcDZOIQGwvPPOOy+mfQDsjwADwPIOHDgQeX2ma2CG9gFwNgIMAMsb+pDG3t5e7d27V7t27dLevXujnkLPwxyB8YNrYABYXkFBQeR1SkqK5s2bF3k/9AjM0D4AzsYRGACWl5KSEtM+APZHgAFgeTU1NZHXZ7oGZmgfAGcjwACwvEAgENM+APZHgAFgeV/72tcir4c+B+mL74f2AXA2AgwAy5s4cWJM+wDYHwEGgOV98sknMe0DYH8EGACWN/iso1j1AbA/AgwAy7vyyitj2gfA/ggwACwvOTk5pn0A7I8AA8Dyenp6YtoHwP4IMAAsr6+vL6Z9AOyPAAPA8pKSkmLaB8D+CDAALK+7uzumfQDsjwADwPI++uijmPYBsD8CDADLu/HGG2PaB8D+CDAALC8lJSWmfQDsjwADwPKMMTHtA2B/BBgAltfb2xvTPgD2R4ABYHmJiYkx7QNgfwQYAJbndo/sT9VI+wDYH//aAQCA7RBgAACA7RBgAFiey+WKaR8A+yPAAAAA2yHAAAAA2yHAAAAA2yHAAAAA2yHAAAAA2yHAAAAA2yHAAAAA2yHAAAAA2yHAAAAA2yHAAAAA2yHAAAAA2yHAAAAA2yHAAAAA2yHAAAAA20mM9wQAjA8TJkzQ7NmzR/1z/uzP/uysv+bDDz9UT0/PKMwGwGghwAAYE7Nnz9Y777xzTt/D5XKdcZvb7f5Sn5GXl6ff/e535zQ3AGOLAANgTHz44YfKy8v7Ul/79ttvKyEhQcYYSdGhZei2gYEBzZ8//0vNDYC9EGAAjImenp4vfZTjxhtv1K9+9asz9hhjdOONN3IkBRgnXJJMvCcxGjwejzo6OpSenq7Ozs54TwfAOerv75fb/dm6g+GOwITDYSUm8n8ywO5Guv9mFRIAW0hMTFQ4HB52jPACjD8EGAC2kZiYqIULF2pgYECSNDAwoIULFxJegHGIf/UA/qScnBx5PJ54T0OSdOLECd1zzz16+eWXdc899+jEiRNfaun0aOjs7FQgEIj3NIBxgQAD4IxycnL00UcfxXsaw3r55ZfjPYXTzJo1ixADjAECDIAzGjzycvfdd+uDDz6I82w+c9555+nSSy/V4cOH9emnn8Z7OpKkr3zlK9q4caNljlQBTmfpAPO9731PjzzyiDIzM/Xuu+/q+9//vt5+++14TwsYd4w+lTHtkjriPRVJ0qefdujDD1viPY0oxrTLyBphChgPLBtg7rjjDq1bt07Lly/Xnj179PDDD2vbtm264oor9Mknn8R7esC4MXHiRElHtPHlB+M9FRs4Eu8JAOOGZQPM3/zN3+jf//3f9cILL0iSli9frptvvll//dd/rX/6p3+K7+SAceSz5xfNkJQZ76nYQAr3nQLGiCUDTFJSkvLz81VeXh7ZZozRjh07VFRUFMeZAeNPVVWVpM9ut9/d3f2lv8/gdStWFKtraViFBIwdSwaYCy+8UImJiQqFQlHbQ6HQH32abXJyslJSUiLvuZAOiI3W1lY999xzMfledXV1Mfk+AOCYG9mVlZWpo6MjUk1NTfGeEgAAGCWWDDDHjh1Tf3+/vF5v1Hav16vm5uZhv6a8vFzp6emRmjZt2lhMFQAAxIElA0xfX5/27dunkpKSyDaXy6WSkpI/egi6t7dXnZ2dUQUAAJzJktfASNK6dev005/+VHv37tVvf/tbPfzww0pNTdXzzz8f76kBAIA4s2yAeeWVV3TRRRfpiSeeUGZmpvbv36+bbrpJLS3WunkVAAAYey5JJt6TGA0ej0cdHR1KT0/ndBIAADYx0v23Ja+BAQAAOBMCDAAAsB0CDAAAsB0CDAAAsB0CDAAAsB0CDAAAsB0CDAAAsB3L3sguVngqNQAA9jHS/bZjA8zgL4CnUgMAYD8ej+eMN7Jz7J14JWnq1KnchRdwII/Ho6amJk2bNo1/44ADeTweHT169Iw9jg4wAJyJR4UA4CJeAABgOwQYAABgOwQYALZz6tQpPf744zp16lS8pwIgTrgGBgAA2A5HYAAAgO0QYAAAgO0QYAAAgO0QYAAAgO0QYADYhs/n0y9/+Us1NTXJGKNbb7013lMCECcEGAC2kZqaqnfffVcrVqyI91QAxJljH+YIwHm2bt2qrVu3xnsaACyAIzAAAMB2CDAAAMB2CDAAAMB2CDAAAMB2CDAAAMB2WIUEwDZSU1OVk5MTeZ+dna25c+eqra1NH3/8cRxnBmCs8TRqALZRXFys6urq07a/8MIL+qu/+quxnxCAuCHAAAAA2+EaGAAAYDsEGAAAYDsEGAAAYDsEGAAAYDsEGAAAYDsEGAAAYDsEGAAAYDsEGAAAYDsEGAAAYDsEGAAAYDsEGAAAYDsEGAAAYDv/H3jDpfiOm9IDAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "freq_dict = {}\n",
    "# for pc in clips:\n",
    "#     freq_dict[pc.word] = freq_dict.get(pc.word, 0) + 1\n",
    "\n",
    "freqs = list(filter(lambda x: 5 < x < 200000, freq_dict.values()))\n",
    "\n",
    "plt.boxplot(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_clips(clips: List[PronunciationClip], min_freq: int = 20,\n",
    "                 max_freq: int = 200) -> List[PronunciationClip]:\n",
    "    freq_dict = {}\n",
    "    for pc in clips:\n",
    "        freq_dict[pc.word] = freq_dict.get(pc.word, 0) + 1\n",
    "    included_words = list(filter(lambda w: min_freq < freq_dict[w] < max_freq,\n",
    "                                 freq_dict.keys()))\n",
    "    return list(filter(lambda c: c.word in included_words, clips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'freq_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[1;32m----> 3\u001b[0m wf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m<\u001b[39m x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m, \u001b[43mfreq_dict\u001b[49m\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(wf)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m words remain after filtering\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocab.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'freq_dict' is not defined"
     ]
    }
   ],
   "source": [
    "wf = dict(filter(lambda x: 50 < x[1] < 300, freq_dict.items()))\n",
    "print(f'{len(wf)} words remain after filtering')\n",
    "\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(wf.keys()))\n",
    "    print('wrote vocabulary file')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "def make_word2ind(clips: List[PronunciationClip]) -> Dict[str, int]:\n",
    "    words2ind = {}\n",
    "    words = set(c.word for c in clips)\n",
    "    for i, w in enumerate(words):\n",
    "        words2ind[w] = i\n",
    "    return words2ind\n",
    "\n",
    "def load_vocab(vocab_file: str = 'vocab.txt') -> List[str]:\n",
    "    vocab = []\n",
    "    with open(vocab_file, 'r') as f:\n",
    "        vocab = f.read().strip().split('\\n')\n",
    "    print(f'Loaded vocabulary {vocab_file} with {len(vocab)} words.')\n",
    "    return vocab\n",
    "\n",
    "def vocab_word2ind(vocab: List[str]) -> Dict[str, int]:\n",
    "    x = {}\n",
    "    for i, w in enumerate(vocab):\n",
    "        x[w] = i\n",
    "    return x\n",
    "\n",
    "def filter_clips_vocab(clips: List[PronunciationClip], vocab: List[str]\n",
    "                 ) -> List[PronunciationClip]:\n",
    "    return list(filter(lambda c: c.word in vocab, clips))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = load_vocab()\n",
    "# clips = load_utterances(20_000, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = os.path.join('parsed_files', 'raw_clips')\n",
    "# utter_path = os.path.join('parsed_files', 'raw_utterance')\n",
    "# os.makedirs(path, exist_ok = True)\n",
    "# os.makedirs(utter_path, exist_ok = True)\n",
    "# for i, c in enumerate(clips):\n",
    "#     og = c.audio.set_channels(1).set_frame_rate(16000)\n",
    "#     og.export(os.path.join(utter_path, f'{c.word}_{i}.wav'), format=\"wav\")\n",
    "#     a = gen_clip(c.audio)\n",
    "#     a.export(os.path.join(path, f'{c.word}_{i}.wav'), format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clips(path: str = 'parsed_files/raw_clips') -> List[PronunciationClip]:\n",
    "    clips = []\n",
    "    for fp in os.listdir(path):\n",
    "        if not fp.endswith('.wav'):\n",
    "            continue\n",
    "        filename = os.path.join(path, fp)\n",
    "        audio = AudioSegment.from_file(filename)\n",
    "        word = fp.split('_')[0]\n",
    "        clips.append(PronunciationClip(word, audio))\n",
    "    return clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocabulary vocab.txt with 237 words.\n"
     ]
    }
   ],
   "source": [
    "clips = load_clips()\n",
    "vocab = load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237 words from 20003 clips\n"
     ]
    }
   ],
   "source": [
    "word2ind = make_word2ind(clips)\n",
    "print(f'{len(word2ind)} words from {len(clips)} clips')\n",
    "word2ind\n",
    "\n",
    "music_clips = make_noise_playlist(os.path.join('audio', 'music'), 200)\n",
    "outdoor_clips = make_noise_playlist(os.path.join('audio', 'outside_rural'))\n",
    "babble_clips = make_noise_playlist(os.path.join('audio', 'babble'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cupof\\AppData\\Local\\Temp\\ipykernel_40724\\2912052705.py:19: RuntimeWarning: invalid value encountered in cast\n",
      "  noise_array = array.array(signal.array_type, np.round(noise_samples).astype(np.int32))\n",
      "c:\\Users\\cupof\\repos\\audio-project\\.venv\\Lib\\site-packages\\pycochleagram-0.1-py3.12.egg\\pycochleagram\\cochleagram.py:135: RuntimeWarning: divide by zero encountered in log10\n",
      "  freqs_to_plot = np.log10(freqs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m o_clip \u001b[38;5;241m=\u001b[39m overlay_noise(outdoor_snr[i], a, o_noise)\n\u001b[0;32m     30\u001b[0m b_clip \u001b[38;5;241m=\u001b[39m overlay_noise(babble_snr[i], a, b_noise)\n\u001b[1;32m---> 31\u001b[0m m_cgram \u001b[38;5;241m=\u001b[39m aa_cgram(\u001b[43maudio_to_cgram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_clip\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     32\u001b[0m o_cgram \u001b[38;5;241m=\u001b[39m aa_cgram(audio_to_cgram(o_clip))\n\u001b[0;32m     33\u001b[0m b_cgram \u001b[38;5;241m=\u001b[39m aa_cgram(audio_to_cgram(b_clip))\n",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m, in \u001b[0;36maudio_to_cgram\u001b[1;34m(audio)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maudio_to_cgram\u001b[39m(audio: AudioSegment) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39marray:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# TODO: find audio input type for cochleagram (8-bit integer PCM?)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(audio\u001b[38;5;241m.\u001b[39mget_array_of_samples())\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcgram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhuman_cochleagram\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSAMPLE_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mN_FILTERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLO_LIM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHI_LIM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownsample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cupof\\repos\\audio-project\\.venv\\Lib\\site-packages\\pycochleagram-0.1-py3.12.egg\\pycochleagram\\cochleagram.py:261\u001b[0m, in \u001b[0;36mhuman_cochleagram\u001b[1;34m(signal, sr, n, low_lim, hi_lim, sample_factor, padding_size, downsample, nonlinearity, fft_mode, ret_mode, strict, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    260\u001b[0m   n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mfloor(erb\u001b[38;5;241m.\u001b[39mfreq2erb(hi_lim) \u001b[38;5;241m-\u001b[39m erb\u001b[38;5;241m.\u001b[39mfreq2erb(low_lim)) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 261\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mcochleagram\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_lim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhi_lim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownsample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnonlinearity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfft_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mret_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\cupof\\repos\\audio-project\\.venv\\Lib\\site-packages\\pycochleagram-0.1-py3.12.egg\\pycochleagram\\cochleagram.py:174\u001b[0m, in \u001b[0;36mcochleagram\u001b[1;34m(signal, sr, n, low_lim, hi_lim, sample_factor, padding_size, downsample, nonlinearity, fft_mode, ret_mode, strict, **kwargs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     temp_sb \u001b[38;5;241m=\u001b[39m apply_envelope_downsample(temp_sb, downsample)\n\u001b[0;32m    172\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# interpret downsample as new sampling rate\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m     temp_sb \u001b[38;5;241m=\u001b[39m \u001b[43mapply_envelope_downsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_sb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpoly\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownsample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m   temp_sb \u001b[38;5;241m=\u001b[39m apply_envelope_nonlinearity(temp_sb, nonlinearity)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\cupof\\repos\\audio-project\\.venv\\Lib\\site-packages\\pycochleagram-0.1-py3.12.egg\\pycochleagram\\cochleagram.py:443\u001b[0m, in \u001b[0;36mapply_envelope_downsample\u001b[1;34m(subband_envelopes, mode, audio_sr, env_sr, invert, strict)\u001b[0m\n\u001b[0;32m    438\u001b[0m   inv_signal, inv_coch \u001b[38;5;241m=\u001b[39m invert_cochleagram_with_filterbank(cochleagram_ref, filts, sr, target_rms\u001b[38;5;241m=\u001b[39mtarget_rms, n_iter\u001b[38;5;241m=\u001b[39mn_iter)\n\u001b[0;32m    440\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inv_signal, inv_coch\n\u001b[1;32m--> 443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_envelope_downsample\u001b[39m(subband_envelopes, mode, audio_sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, env_sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, invert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    444\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Apply a downsampling operation to cochleagram subband envelopes.\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \n\u001b[0;32m    446\u001b[0m \u001b[38;5;124;03m  The `mode` argument can be a predefined downsampling type from\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;124;03m      downsampled with `mode`.\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m    480\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import random\n",
    "\n",
    "# all_clips = []\n",
    "# all_targets = []\n",
    "\n",
    "\n",
    "# # music is -6dB mean, other noise is -3dB mean. all with 2dB variance\n",
    "# np.random.seed(123)\n",
    "# N = len(clips)\n",
    "# music_snr = np.random.normal(-6, 2, N)\n",
    "# outdoor_snr = np.random.normal(-3, 2, N)\n",
    "# babble_snr = np.random.normal(-3, 2, N)\n",
    "\n",
    "# m_path = os.path.join('parsed_files', 'music')\n",
    "# o_path = os.path.join('parsed_files', 'outdoor')\n",
    "# b_path = os.path.join('parsed_files', 'babble')\n",
    "\n",
    "# os.makedirs(m_path, exist_ok = True)\n",
    "# os.makedirs(o_path, exist_ok = True)\n",
    "# os.makedirs(b_path, exist_ok = True)\n",
    "\n",
    "# random.seed(123)\n",
    "# for i, clip in enumerate(clips):\n",
    "#     a = gen_clip(clip.audio)\n",
    "#     m_noise = random.choice(music_clips)\n",
    "#     o_noise = random.choice(outdoor_clips)\n",
    "#     b_noise = random.choice(babble_clips)\n",
    "#     m_clip = overlay_noise(music_snr[i], a, m_noise)\n",
    "#     o_clip = overlay_noise(outdoor_snr[i], a, o_noise)\n",
    "#     b_clip = overlay_noise(babble_snr[i], a, b_noise)\n",
    "#     m_cgram = aa_cgram(audio_to_cgram(m_clip))\n",
    "#     o_cgram = aa_cgram(audio_to_cgram(o_clip))\n",
    "#     b_cgram = aa_cgram(audio_to_cgram(b_clip))\n",
    "#     all_clips.append(m_cgram)\n",
    "#     all_clips.append(o_cgram)\n",
    "#     all_clips.append(b_cgram)\n",
    "\n",
    "#     fname = f'{clip.word}_{i}'\n",
    "\n",
    "#     m_clip.export(os.path.join(m_path, fname + '.wav'), format=\"wav\")\n",
    "#     o_clip.export(os.path.join(o_path, fname + '.wav'), format=\"wav\")\n",
    "#     b_clip.export(os.path.join(b_path, fname + '.wav'), format=\"wav\")\n",
    "#     np.save(os.path.join(m_path, fname + '.npy'), m_cgram)\n",
    "#     np.save(os.path.join(o_path, fname + '.npy'), m_cgram)\n",
    "#     np.save(os.path.join(b_path, fname + '.npy'), m_cgram)\n",
    "\n",
    "#     word_id = word2ind[clip.word]\n",
    "#     all_targets.extend([word_id, word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocabulary vocab.txt with 237 words.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[4.49515113e+06, 9.24959008e+06, 8.73088732e+06, ...,\n",
       "          1.08147777e+07, 1.00926726e+07, 1.02333184e+07],\n",
       "         [4.40310368e+06, 9.66684687e+06, 1.01305830e+07, ...,\n",
       "          1.08153390e+07, 9.45851343e+06, 9.54587546e+06],\n",
       "         [2.40321472e+06, 6.95156203e+06, 1.05947240e+07, ...,\n",
       "          1.37136710e+07, 1.09313796e+07, 7.45638400e+06],\n",
       "         ...,\n",
       "         [2.32944785e+08, 4.92995008e+08, 4.89250316e+08, ...,\n",
       "          4.45353405e+08, 4.33913969e+08, 5.04644898e+08],\n",
       "         [2.39492548e+08, 5.01244281e+08, 5.01641453e+08, ...,\n",
       "          4.54731135e+08, 4.44501887e+08, 5.14464738e+08],\n",
       "         [2.37992132e+08, 4.95977835e+08, 4.98265576e+08, ...,\n",
       "          4.50471129e+08, 4.42047333e+08, 5.10014339e+08]],\n",
       " \n",
       "        [[5.55523274e+06, 1.64851955e+07, 2.63544944e+07, ...,\n",
       "          2.30421203e+07, 1.49149255e+07, 8.49258943e+06],\n",
       "         [6.66134717e+06, 2.30084065e+07, 3.31603278e+07, ...,\n",
       "          2.31576260e+07, 1.96102252e+07, 6.22506129e+06],\n",
       "         [8.33596930e+06, 2.78836763e+07, 3.86100892e+07, ...,\n",
       "          2.41377777e+07, 2.44508955e+07, 8.97383059e+06],\n",
       "         ...,\n",
       "         [2.19871653e+08, 4.73906793e+08, 4.72774972e+08, ...,\n",
       "          4.49273741e+08, 4.75740111e+08, 5.58128817e+08],\n",
       "         [2.26003415e+08, 4.85916902e+08, 4.83714203e+08, ...,\n",
       "          4.62439072e+08, 4.89404863e+08, 5.70764116e+08],\n",
       "         [2.25109652e+08, 4.82377114e+08, 4.81022681e+08, ...,\n",
       "          4.60362234e+08, 4.86922234e+08, 5.65994676e+08]],\n",
       " \n",
       "        [[1.76284862e+07, 6.40807974e+06, 3.12500876e+07, ...,\n",
       "          1.63937829e+07, 3.75988162e+07, 4.82283209e+07],\n",
       "         [2.10509445e+07, 1.19466616e+07, 3.81285431e+07, ...,\n",
       "          1.88307985e+07, 3.41660021e+07, 5.46441097e+07],\n",
       "         [2.39686072e+07, 1.92256977e+07, 4.62192067e+07, ...,\n",
       "          1.96600354e+07, 2.96735527e+07, 5.75714439e+07],\n",
       "         ...,\n",
       "         [1.90988616e+08, 4.71607044e+08, 4.31820096e+08, ...,\n",
       "          4.63101937e+08, 4.37761007e+08, 4.72236650e+08],\n",
       "         [1.95711701e+08, 4.81672711e+08, 4.43193089e+08, ...,\n",
       "          4.76761987e+08, 4.48148704e+08, 4.84532257e+08],\n",
       "         [1.94556079e+08, 4.77246381e+08, 4.40843118e+08, ...,\n",
       "          4.74095182e+08, 4.45523329e+08, 4.80514948e+08]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[7.42793767e+06, 1.76091038e+07, 2.21024400e+07, ...,\n",
       "          1.40638102e+07, 1.78319123e+07, 1.67216610e+07],\n",
       "         [6.84568908e+06, 1.85538099e+07, 2.13106685e+07, ...,\n",
       "          1.34861963e+07, 1.86645860e+07, 1.66201810e+07],\n",
       "         [7.14989780e+06, 1.76289936e+07, 2.17785233e+07, ...,\n",
       "          1.34496065e+07, 1.81551366e+07, 1.71195497e+07],\n",
       "         ...,\n",
       "         [2.21258412e+08, 4.16851696e+08, 4.23755921e+08, ...,\n",
       "          4.55277279e+08, 4.24954982e+08, 4.64794513e+08],\n",
       "         [2.29275967e+08, 4.28058048e+08, 4.34187089e+08, ...,\n",
       "          4.66096618e+08, 4.33892711e+08, 4.72895580e+08],\n",
       "         [2.28592196e+08, 4.25600524e+08, 4.31213919e+08, ...,\n",
       "          4.62634585e+08, 4.29564792e+08, 4.68136746e+08]],\n",
       " \n",
       "        [[1.92378835e+07, 3.62596780e+07, 3.12867480e+07, ...,\n",
       "          2.50036182e+07, 9.51778597e+06, 3.75751550e+07],\n",
       "         [1.99733411e+07, 3.65822488e+07, 2.97649503e+07, ...,\n",
       "          2.64776590e+07, 8.74539521e+06, 3.89656935e+07],\n",
       "         [1.94431548e+07, 3.62777007e+07, 2.75718007e+07, ...,\n",
       "          2.72365284e+07, 9.42349203e+06, 3.77603985e+07],\n",
       "         ...,\n",
       "         [2.20499098e+08, 5.17915079e+08, 4.77517155e+08, ...,\n",
       "          4.71971356e+08, 4.47571799e+08, 4.97491990e+08],\n",
       "         [2.26483869e+08, 5.31948441e+08, 4.89441468e+08, ...,\n",
       "          4.85819335e+08, 4.59562815e+08, 5.10324904e+08],\n",
       "         [2.25312484e+08, 5.28300207e+08, 4.86204735e+08, ...,\n",
       "          4.83081445e+08, 4.57151428e+08, 5.06588959e+08]],\n",
       " \n",
       "        [[6.71033466e+06, 1.36046805e+07, 2.22550226e+07, ...,\n",
       "          2.08930308e+07, 8.33389688e+06, 1.32702831e+07],\n",
       "         [8.17862215e+06, 1.00950592e+07, 2.62938645e+07, ...,\n",
       "          2.28721838e+07, 1.30696797e+07, 1.86063852e+07],\n",
       "         [9.88221240e+06, 8.11635922e+06, 3.02414021e+07, ...,\n",
       "          2.46886725e+07, 1.75706418e+07, 2.38963809e+07],\n",
       "         ...,\n",
       "         [2.06677971e+08, 5.27923970e+08, 4.49384748e+08, ...,\n",
       "          4.67522116e+08, 4.02174242e+08, 5.02204309e+08],\n",
       "         [2.11309194e+08, 5.38976490e+08, 4.60160771e+08, ...,\n",
       "          4.78792515e+08, 4.11614824e+08, 5.15097263e+08],\n",
       "         [2.09215816e+08, 5.34524250e+08, 4.56568707e+08, ...,\n",
       "          4.75034774e+08, 4.08799783e+08, 5.11271928e+08]]]),\n",
       " array([154, 154, 154, ..., 160, 160, 160]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_cochleagrams(path: os.PathLike, word2ind: Dict[str, int]\n",
    "                      ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    data = []\n",
    "    targets = []\n",
    "    for f in os.listdir(path):\n",
    "        w = f.split('_')[0]\n",
    "        if not f.endswith(\".npy\") or w not in word2ind:\n",
    "            continue\n",
    "        x = np.load(os.path.join(path, f))\n",
    "        data.append(x)\n",
    "        targets.append(word2ind[w])\n",
    "    return np.array(data), np.array(targets)\n",
    "\n",
    "vocab = load_vocab()\n",
    "word2ind = vocab_word2ind(vocab)\n",
    "load_cochleagrams('parsed_files/babble', word2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
